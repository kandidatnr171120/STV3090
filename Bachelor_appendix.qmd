---
title: "Bachelor appendix"
author: "Kandidatnummer: 171120"
format: pdf
editor: visual
execution: 
  warnings: false
---

## Skraping av URL

```{r}
library(tidyverse)
library(rvest)
library(dplyr)
library(httr)
library(dplyr)
library(lubridate)
library(openxlsx)
library(tidymodels)
library(textrecipes)
library(ranger)
```

```{r, eval=FALSE}

setwd("~/Documents/STV3090 - Bachelor oppgave")


url1 <- "https://iz.ru/search?type=0&prd=0&from="
url2 <- "&text=Ukraine&date_from=&date_to=2022-02-16&sort=0"
# Siden nettside oppsettet har flere sider byttes ut sidetall delen i koden og kjøres på hvert sidetall, hver side linker til 10 URLer 
range <- seq(0, 4000, 1)

#Nettsiden ikke tillater skraping må det gjennomføres i flere iterasjon for å ikke bli blokkert
range_one <- range[1:100]
range_two <- range[101:200]
range_three <- range[201:300]
range_4 <- range[301:400]
range_5 <- range[401:500]
range_6 <- range[501:600]
range_7 <- range[601:700]
range_8 <- range[701:800]
range_9 <- range[801:900]
range_10 <- range[901:1000]
range_11 <- range[1001:1100]
range_12 <- range[1101:1200]
range_13 <- range[1201:1300]
range_20 <- range[2001:2100]


#del 2: mars 2022- februar 2023/Funker ikke 

url1 <- "https://iz.ru/search?type=0&prd=0&from="
url2 <- "&text=Украина&date_from=2022-02-24&date_to=2023-02-28&sort=0"
range <- seq(0, 2, 1)

url_list <- character()  


# Iterasjoner hvor dette byttes hvilken "range" som brukes
for (page in range) {
  url <- paste0(url1, page, url2)
  url_list <- c(url_list, url)  # Append the URL to the list
}

all_urls <- c()

for (url in url_list) {
  webpage <- read_html(url)
  
  # Extract the div nodes containing the titles and URLs
  page_titles <- html_nodes(webpage, "div.view-search__title a")
  
  # Extract the URLs using the href attribute
  page_urls <- html_attr(page_titles, "href")
  
  # Append the page URLs to the overall vector
  all_urls <- c(all_urls, page_urls)
  
  # Legger til tilfeldig forsinkelse i henting av data
  Sys.sleep(runif(1, 2, 8))
  }

# lage er dataframe med URLene 
urls_data <- data.frame(URL = all_urls)

# Sjekker lengen av datasettet for å teste om koden fuker 
cat("URLs length:", length(all_urls), "\n")

write.csv(urls_data, "urls_data_final.csv", row.names = FALSE)


```

## Stacker URL dataene i et datasett

```{r, eval = FALSE}


# Henter inn URL datasettene 
urls_data_one <- read.csv("urls_data_one.csv")
urls_data_two <- read.csv("urls_data_two.csv")
urls_data_three <- read.csv("urls_data_three.csv")
urls_data_4 <- read.csv("urls_data_4.csv")
urls_data_5 <- read.csv("urls_data_5.csv")
urls_data_6 <- read.csv("urls_data_6.csv")
urls_data_7 <- read.csv("urls_data_7.csv")
urls_data_8 <- read.csv("urls_data_8.csv")
urls_data_9 <- read.csv("urls_data_9.csv")
urls_data_10 <- read.csv("urls_data_10.csv")

# Kobler datasettene sammen 
stacked_data <- bind_rows(urls_data_one, urls_data_two, urls_data_three, urls_data_4, urls_data_5, urls_data_6, urls_data_7, urls_data_8, urls_data_9, urls_data_10)

write.csv(stacked_data, "Stacked_URL.csv", row.names = FALSE)

```

## Skraper artiklene ut ifra URLene

```{r, eval =FALSE}

URLs_data <- read_csv("Stacked_URL.csv")


# Setter seed for å kunne replikere 
set.seed(34)

#Fordi datasetter er såpass stort URL sidene skrapes i flere omganger å omgå sikkerhets mekansimene i nettsiden.

# Randomly select 200 observations from URLs_data
random_indices <- sample(seq_len(nrow(URLs_data)), size = 200, replace = FALSE)

# Create a new dataset with the randomly selected observations
stacked_data <- URLs_data[random_indices, ]
# Create a new dataset with the remaining observations after sampling
remaining_data <- URLs_data[-random_indices, ]

# Randomly select 200 observations from remaining_data
random_indices2 <- sample(seq_len(nrow(remaining_data)), size = 200, replace = FALSE)

# Create a new dataset with the randomly selected observations
stacked_data2 <- remaining_data[random_indices2, ]
# Create a new dataset with the remaining observations after sampling
remaining_data2 <- remaining_data[-random_indices2, ]

# Randomly select 200 observations from remaining_data2
random_indices3 <- sample(seq_len(nrow(remaining_data2)), size = 200, replace = FALSE)

# Create a new dataset with the randomly selected observations
stacked_data3 <- remaining_data2[random_indices3, ]
# Create a new dataset with the remaining observations after sampling
remaining_data3 <- remaining_data2[-random_indices3, ]

# Randomly select 200 observations from remaining_data3
random_indices4 <- sample(seq_len(nrow(remaining_data3)), size = 200, replace = FALSE)

# Create a new dataset with the randomly selected observations
stacked_data4 <- remaining_data3[random_indices4, ]
# Create a new dataset with the remaining observations after sampling
remaining_data4 <- remaining_data3[-random_indices4, ]

# Randomly select 200 observations from remaining_data3
random_indices5 <- sample(seq_len(nrow(remaining_data4)), size = 200, replace = FALSE)
# Create a new dataset with the randomly selected observations
stacked_data5 <- remaining_data4[random_indices5, ]
# Create a new dataset with the remaining observations after sampling
remaining_data5 <- remaining_data4[-random_indices5, ]

# Randomly select 200 observations from remaining_data3
random_indices6 <- sample(seq_len(nrow(remaining_data5)), size = 200, replace = FALSE)
# Create a new dataset with the randomly selected observations
stacked_data6 <- remaining_data5[random_indices6, ]
# Create a new dataset with the remaining observations after sampling
remaining_data6 <- remaining_data5[-random_indices6, ]

# Randomly select 200 observations from remaining_data3
random_indices7 <- sample(seq_len(nrow(remaining_data6)), size = 200, replace = FALSE)
# Create a new dataset with the randomly selected observations
stacked_data7 <- remaining_data6[random_indices7, ]
# Create a new dataset with the remaining observations after sampling
remaining_data7 <- remaining_data6[-random_indices7, ]

# Randomly select 200 observations from remaining_data3
random_indices8 <- sample(seq_len(nrow(remaining_data7)), size = 200, replace = FALSE)
# Create a new dataset with the randomly selected observations
stacked_data8 <- remaining_data7[random_indices8, ]
# Create a new dataset with the remaining observations after sampling
remaining_data8 <- remaining_data7[-random_indices8, ]

# Randomly select 200 observations from remaining_data3
random_indices9 <- sample(seq_len(nrow(remaining_data8)), size = 200, replace = FALSE)
# Create a new dataset with the randomly selected observations
stacked_data9 <- remaining_data8[random_indices9, ]
# Create a new dataset with the remaining observations after sampling
remaining_data9 <- remaining_data8[-random_indices9, ]


# Lager tomme vektorer for å sette de skrapede data et sted
dates <- vector()
contents <- vector()




#Her er det også brukt en VPN for å få nettsiden til å tro at mennesker fra forskjellige land klikker på nettsiden 
# Definerer ulike agenter for mimikere ulike nettlesere
user_agents <- c(
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
  "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.5 Safari/605.1.15",
  "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0"
)

# Initialize vectors to store scraped data
dates <- c()
contents <- c()

# hvilket datasett som brukes blir byttet ut hver gang med "stacked_data" 1-9 
for (url in 1:length(stacked_data9$URL)) {
  tryCatch({
    # Set the user-agent randomly from predefined list
    user_agent <- sample(user_agents, 1)
    
    # Make the request using a session
    webpage <- read_html(GET(stacked_data9$URL[url], user_agent(user_agent)))
    
    # Hentet ut dato og innhold gjennom CSS selectors
    date <- html_text(html_nodes(webpage, "#block-purple-content > div > div.article_page__left__top > div > div.article_page__left__top__time > div.article_page__left__top__time__label > div > time"))
    content <- html_text(html_nodes(webpage, "#block-purple-content > div > div.text-article > article"))
    
    # Clean up the text
    content <- gsub("[\r\n\t]", "", content)
    date <- gsub("[\r\n\t]", "", date)
    
    # Append the scraped data to the vectors
    dates <- c(dates, date)
    contents <- c(contents, content)
    
    # Legger til forsknikelser 
    Sys.sleep(runif(1, 2, 7))
    if(url %% 20 == 0){
      # Lengre forsinkelse for å være høflig :) 
      Sys.sleep(60*4)
    }
  }, error = function(e) {
    message(paste0("Error with URL ", url, ": ", e$message, ". Moving on..."))
    # Sleep longer if an error occurred to prevent possible further blocking
    Sys.sleep(runif(1, 110, 400))
  })
}

# Lage et datasett ut ifra hentet data
transcripts_stack_9 <- data.frame(Date = dates, Content = contents)


View(transcripts_stack_9)


write.csv(transcripts_stack_9, "stack_sample9_transcript.csv", row.names = FALSE)


```

Trekker ut et av datasettene for å kode manuelt

## Kobler sammen artikkelene, oversetter og fikser dato

```{r, eval=FALSE}
#Kobler sammen artiklene :) 
#Jeg har allerede brukt stack_sample2_transcript for oversettlelse og treningsdata 


stack_sample1_transcript <- read_csv("stack_sample1_transcript.csv")
stack_sample3_transcript <- read_csv("stack_sample3_transcript.csv")
stack_sample4_transcript <- read_csv("stack_sample4_transcript.csv")
stack_sample5_transcript <- read_csv("stack_sample5_transcript.csv")
stack_sample6_transcript <- read_csv("stack_sample6_transcript.csv")
stack_sample7_transcript <- read_csv("stack_sample7_transcript.csv")
stack_sample8_transcript <- read_csv("stack_sample8_transcript.csv")

stacked_articles <- bind_rows(stack_sample1_transcript,stack_sample3_transcript,stack_sample4_transcript, stack_sample5_transcript, stack_sample6_transcript, stack_sample7_transcript, stack_sample8_transcript)


write.xlsx(stacked_articles, file = "artikler_for_oversettelse.xlsx")

write.xlsx(stack_sample2_transcript, file = "oversettelse.xlsx", sheetName = "Data")

#Henter det oversatte datasettet
artikler_for_oversettelse_engelsk_ <- read_excel("artikler_for_oversettelse_engelsk .xlsx")


#Fikser datoene til et brukbart format 
artikler_for_oversettelse_engelsk_$Date <- gsub(" [0-9]{2}:[0-9]{2}( p\\.m\\.)?", "", artikler_for_oversettelse_engelsk_$Date)
artikler_for_oversettelse_engelsk_$Date <- gsub(",", "", artikler_for_oversettelse_engelsk_$Date)
artikler_for_oversettelse_engelsk_$Date <- gsub("\\s*[Pp][.]?[Mm]\\.*", "", artikler_for_oversettelse_engelsk_$Date)


artikler_for_oversettelse_engelsk_$DateParsed <- dmy(artikler_for_oversettelse_engelsk_$Date)

# Replace non-matched dates with mdy parsing results
non_matched <- is.na(artikler_for_oversettelse_engelsk_$DateParsed)
artikler_for_oversettelse_engelsk_$DateParsed[non_matched] <- mdy(artikler_for_oversettelse_engelsk_$Date[non_matched])

sum(is.na(artikler_for_oversettelse_engelsk_$DateParsed))

#Lagrer datasett så jeg slipper å kjøre koden igjen 
write.csv(artikler_for_oversettelse_engelsk_, "IZ_hele.csv", row.names = FALSE)
```

## Trener modellen

```{r, eval=FALSE}

#write.xlsx(stack_sample2_transcript, file = "oversettelse.xlsx")
#write.xlsx(stack_sample2_transcript, file = "oversettelse.xlsx", sheetName = "Data")
library(openxlsx)
library(tidymodels)
library(textrecipes)
library(ranger)
library(readxl)
IZ_kodet <- read_excel("~/Documents/STV3090 - Bachelor oppgave/IZ_kodet.xlsx")
oversettelse <- IZ_kodet


oversettelse$Positivnegativ <- as.factor(oversettelse$Positivnegativ)

# Data preprossering
oversettelse$Positivnegativ <- ifelse(oversettelse$Positivnegativ == 9, 0, as.character(oversettelse$Positivnegativ))
oversettelse$Positivnegativ <- factor(oversettelse$Positivnegativ)

# Splitting
set.seed(3.14)
data_split <- initial_split(oversettelse, prop = 0.8)
train_data <- training(data_split)
test_data  <- testing(data_split)

# Preprossering
text_recipe <- recipe(Positivnegativ ~ Content, data = train_data) %>%
  step_tokenize(Content) %>%
  step_stopwords(Content) %>%
  step_tokenfilter(Content, max_tokens = 1000) %>%
  step_tfidf(Content)


#Random Forest modell 
rf_spec <- rand_forest(mode = "classification", trees = 500) %>%
  set_engine("ranger", importance = 'permutation')

# Create the workflow
rf_wflow <- workflow() %>%
  add_recipe(text_recipe) %>%
  add_model(rf_spec)

# Fit the model
rf_fit <- fit(rf_wflow, data = train_data)


# Predict and evaluate the model
results <- predict(rf_fit, test_data) %>%
  bind_cols(test_data)

metrics <- metrics(results, truth = Positivnegativ, estimate = .pred_class)


print(metrics)

conf_mat <- conf_mat(results, truth = Positivnegativ, estimate = .pred_class) %>% 
  autoplot(type = "heatmap") + 
  ggtitle("Confusion matrix: Prediksjon over positive og negative artikler")


print(conf_mat)


#Her predikter jeg det u-klassifiserte datasettet 

# Use the trained model to predict the target variable
predicted_results <- predict(rf_fit, new_data = koblet_IZ)

# Optionally, bind the predicted values with the "koblet_IZ" dataset
predicted_results <- bind_cols(koblet_IZ, predicted_results)

# Print the predicted results
print(predicted_results)

write.csv(predicted_results, "predicted_results.csv", row.names = FALSE)

#Nå er det laget et dataset som heter "predicted_results" som er det samme som "koblet_IZ" bare med en ny variabel som heter ".pred_class" med variablene 1 (som betyr positiv) og 0 (som betyr negativ)
```

## Graf over russiske tap

```{r, echo=FALSE}
russia_losses_personnel <- read_csv("russia_losses_personnel.csv")

russia_losses_personnel$losses_per_day <- c(russia_losses_personnel$personnel[1], diff(russia_losses_personnel$personnel))



# Konverter date til riktig format
russia_losses_personnel$date <- ymd(russia_losses_personnel$date)

# Lag en ny kolonne for måned
russia_losses_personnel$month <- format(russia_losses_personnel$date, "%Y-%m")

# Aggreger antall tap per måned
monthly_losses <- aggregate(losses_per_day ~ month, data = russia_losses_personnel, FUN = sum)

monthly_losses_23 <- monthly_losses %>% filter(month > "2023-02")

# Vis månedlige tap som søyler
barplot(monthly_losses_23$losses_per_day, names.arg = monthly_losses_23$month, xlab = "Måned", ylab = "Antall russiske tap", main = "Antall russiske personelltap tap per måned")

ggplot(monthly_losses_23, aes(x = month, y = losses_per_day)) + 
  geom_line(aes(group = 1)) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Graf over artikkelene

```{r}
library(tidyverse)
library(dplyr)
library(lubridate)
library(readxl)
library(gridExtra)
library(grid)

predicted_results <- read_csv("predicted_results.csv")
IZ_kodet <- read_excel("IZ_kodet.xlsx")
koblet_IZ <- read_csv("koblet_IZ.csv")


IZ_kodet$Positivnegativ <- ifelse(IZ_kodet$Positivnegativ == 9, 0, as.character(IZ_kodet$Positivnegativ))
IZ_kodet$Positivnegativ <- as.numeric(IZ_kodet$Positivnegativ)


IZ_kodet$Positivnegativ <- factor(IZ_kodet$Positivnegativ,
                                  levels = c(1, 0),
                                  labels = c("Positive", "Negative"))


IZ_kodet$Putin_Mentioned <- ifelse(grepl("Putin", IZ_kodet$Content, ignore.case = TRUE), "Yes", "No")

koblet_IZ$Putin_Mentioned <- ifelse(grepl("Putin", koblet_IZ$Content, ignore.case = TRUE), "Yes", "No")

library(ggplot2)

# Filtrer ut artikler hvor Putin ikke er nevnt 
IZ_kodet_mentioned <- subset(IZ_kodet, Putin_Mentioned == "Yes")


ggplot(IZ_kodet_mentioned, aes(x = Positivnegativ, fill = Positivnegativ)) +
  geom_bar() +
  labs(x = "Article Sentiment", y = "Articles mentioning Putin",
       title = "Count of Positive vs. Negative Articles Mentioning Putin") +
  scale_fill_manual(values = c("Positive" = "blue", "Negative" = "red")) +
  theme_minimal() +
  theme(legend.position = "none") 

koblet_IZ <- read_csv("koblet_IZ.csv")
#koblet_IZ <- bind_rows(IZ_kodet, IZ_hele)
#write.csv(koblet_IZ, "koblet_IZ.csv", row.names = FALSE)


koblet_IZ$Putin_Mentioned <- ifelse(grepl("Putin", koblet_IZ$Content, ignore.case = TRUE), "Yes", "No")


koblet_IZ$month_year <- format(koblet_IZ$DateParsed, "%Y-%m")

# Create a new dataframe with the count of "yes" answers per month_year
monthly_yes_counts <- koblet_IZ %>%
  filter(Putin_Mentioned == "Yes") %>%
  group_by(month_year) %>%
  summarise(count = n())

# Adjust the count data to be ordered by date
monthly_yes_counts$month_year <- factor(monthly_yes_counts$month_year, levels = monthly_yes_counts$month_year[order(as.Date(paste0(monthly_yes_counts$month_year, "-01")))])

# Plot
ggplot(monthly_yes_counts, aes(x = month_year, y = count)) +
  geom_bar(stat="identity", fill = "blue") +
  xlab("Month") +
  ylab("Count of 'Yes' Mentions") +
  ggtitle("Number of 'Yes' Mentions of Putin per Month") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) # Rotate x labels to avoid overlap


trie <- left_join(monthly_yes_counts, monthly_losses, by = c("month_year" = "month"))

ggplot(trie, aes(x = trie$count, y = trie$losses_per_day)) + 
  geom_point() + 
  geom_smooth(method = "lm")


# ny plot 
predicted_results$.pred_class <- factor(predicted_results$.pred_class,
                                  levels = c(1, 0),
                                  labels = c("Positive", "Negative"))


IZ_pred_mentioned <- subset(predicted_results, Putin_Mentioned == "Yes")




PutinPlot12 <- ggplot(IZ_pred_mentioned, aes(x = .pred_class, fill = .pred_class)) +
  geom_bar() +
  labs(x = "Sentiment", y = "Artikler som nevner Putin",
       subtitle = "Antall") +
  scale_fill_manual(values = c("Negative" = "red", "Positive" = "blue")) +
  theme_bw() +
  theme(legend.position = "none") 

sentiment_percentages <- IZ_pred_mentioned %>%
  group_by(.pred_class) %>%
  summarise(percentage = n() / nrow(IZ_pred_mentioned) * 100)


percentage_dataset <- koblet_IZ %>%
  # Group by Positivnegativ
  group_by(Positivnegativ) %>%
  # Get the count of each Putin_mentioned within each Positivnegativ class
  count(Putin_Mentioned) %>%
  # Group by Positivnegativ again to ensure the percentage calculation is done within each group
  group_by(Positivnegativ) %>%
  # Calculate percentages
  mutate(Percentage = n / sum(n) * 100) %>%
  # Select only the relevant columns
  select(Positivnegativ, Putin_Mentioned, Percentage)

percentage_dataset <- subset(percentage_dataset, Putin_Mentioned == "Yes")

percentage_dataset <- na.omit(percentage_dataset)


# Plot the percentage
Putinplot34 <- ggplot(percentage_dataset, aes(x = Positivnegativ, y = Percentage, fill = Positivnegativ)) +
  geom_bar(stat = "identity") +
   labs(x = "Sentiment", y = "Prosent artikler som nevner Putin",
       subtitle = "Prosent") +
  scale_fill_manual(values = c("Negative" = "red", "Positive" = "blue")) +
  theme_bw() +
  theme(legend.position = "none")


Putin_caption <- "Hentet fra den russiske statseide avisen Izvestia i perioden Februar 2023 - Mars 2024"


title_size <- 14
subtitle_size <- 10
caption_size <- 10

grid.arrange(
  PutinPlot12, Putinplot34, 
  ncol = 2,
  top = textGrob("Predikert positive og negative artikler som nevner Putin", gp = gpar(fontsize = title_size, fontface = "bold")),
  bottom = textGrob(Putin_caption, gp = gpar(fontsize = caption_size))
)





predicted_results_clean <- predicted_results[!is.na(predicted_results$month_year), ]

ggplot(predicted_results_clean, aes(x = month_year, color = as.factor(.pred_class), group = as.factor(.pred_class))) +
  geom_line(stat = "count") +
  labs(x = "", y = "Antall artikler", color = "Sentiment") +
  scale_color_manual(values = c("Negative" = "red", "Positive" = "blue"), labels = c("Postive", "Negative")) +
  theme_bw() +
  ggtitle("Antall positive og negative nyhetsreportasjer om krigen i Ukraina", subtitle = "Februar 2023 - Mars 2024") + 
  labs(caption = "Hentet fra den russiske statseide avisen Izvestia" ) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



# Calculate the percentage of positive and negative articles for each month
data_percentages <- predicted_results_clean %>%
  group_by(month_year, .pred_class) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)



# Assuming you have the percentages calculated as data_percentages from the previous step

# Filter the data to only include negative articles
data_negative <- data_percentages[data_percentages$.pred_class == "Negative", ]

ggplot(data_negative, aes(x = month_year, y = percentage, group = "Negative")) +
  geom_line(color = "red") +
  labs(x = "", y = "Prosent av negative artikler") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ggtitle("Prosentandel negative nyhetsreportasjer om krigen i Ukraina", subtitle = "Februar 2023 - Mars 2024") + 
  labs(caption = "Hentet fra den russiske statseide avisen Izvestia")

Negative_losses <- left_join(data_negative, monthly_losses, by = c("month_year" = "month"))

# Calculate scale_factor with a check to avoid division by zero or NA values
max_losses_per_day <- max(Negative_losses$losses_per_day, na.rm = TRUE)
max_percentage <- max(Negative_losses$percentage, na.rm = TRUE)

# Avoid division by zero or if max_percentage is NA (which shouldn't normally occur)
if(!is.na(max_percentage) && max_percentage > 0) {
  scale_factor <- max_losses_per_day / max_percentage
} else {
  scale_factor <- 1 # Default to 1 if the above condition is not met
}

# plot
ggplot(data = Negative_losses, aes(x = month_year)) +
  geom_col(aes(y = losses_per_day), fill = "skyblue") + 
  geom_line(aes(y = percentage * scale_factor, group = 1), color = "red") + 
  scale_y_continuous(
    name = "Russiske personell tap per måned",
    sec.axis = sec_axis(~ . / scale_factor, name = "Percentage")
  ) +
  theme_minimal() +
  labs(
    x = "",
    y = "Russiske personell tap per måned",
    title = "Russiske personelltap og negative nyheter"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(data = Negative_losses, aes(x = month_year)) +
  geom_col(aes(y = losses_per_day, fill = "Losses per Day")) + # Assign a fill aesthetic for "Losses per Day"
  geom_line(aes(y = percentage * scale_factor, color = "Percentage", group = 1)) + # Assign a color aesthetic for "Percentage"
  scale_fill_manual(name = "", values = "skyblue", labels = "Personell\n tap") + # Manual scale for fill
  scale_color_manual(name = "", values = "red", labels = "Prosent \nnegative \nnyheter") + # Manual scale for color
  scale_y_continuous(
    name = "Russiske personell tap per måned",
    sec.axis = sec_axis(~ . / scale_factor, name = "Prosent")
  ) +
  theme_minimal() +
  labs(
    x = "",
    y = "Russiske personell tap per måned",
    title = "Russiske personelltap og negative nyheter", 
    caption = "Kilde: Ivanouk 2022 og Izvestia.ru"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(fill = guide_legend(order = 1), color = guide_legend(order = 2)) # Adjust the order of legend items

 ggplot(Negative_losses, aes(x = losses_per_day, y = percentage)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_minimal() + 
  labs( x = "soldater tatt ut av tjeneste", 
        y = "Prosentandel negative nyheter",
        title = "Prosentandel negative nyheter og antall russiske soldater tapt", 
       caption = "Kilde: Ivanouk 2022 og Izvestia.ru" )


```

```{r}
#Graf over støttepakker til Ukraina
library(tidyverse)

#Laster de inn fordi koden ble ødelagt når jeg skulle rendre av en eller annen rar grunn
filtered_data <- read_csv("filtered_data.csv")
joined_data <- read_csv("joined_data.csv")

#joined_data <- Negative_losses %>%
 # left_join(filtered_data, by = c("month_year" = "month_year_to_join"))


Kiel_pengestøtte_ <- read_excel("Kiel-pengestøtte .xlsx")

Kiel_pengestøtte_$`Total aid, allocated (€ billion)` <- format(Kiel_pengestøtte_$`Total aid, allocated (€ billion)`, scientific = FALSE)


Kiel_pengestøtte_$Month <- gsub('"', '', Kiel_pengestøtte_$Month)
Kiel_pengestøtte_$Month <- as.Date(Kiel_pengestøtte_$Month, format="%d/%m/%Y")

ggplot(Kiel_pengestøtte_, aes(x = Kiel_pengestøtte_$Month, y = Kiel_pengestøtte_$`Total aid, allocated (€ billion)`)) + 
  geom_point()


# Filterer data
#filtered_data <- Kiel_pengestøtte_ %>%
# filter(Month > as.Date("2023-01-31"))



filtered_data$`Total aid, allocated (€ billion)` <- as.numeric(gsub(",", ".", filtered_data$`Total aid, allocated (€ billion)`))
filtered_data$`Military aid, allocated (€ billion)` <- as.numeric(gsub(",", ".", filtered_data$`Military aid, allocated (€ billion)`))


# Create the plot
ggplot(filtered_data, aes(x = Month, y = `Total aid, allocated (€ billion)`)) +
  geom_bar(stat = "identity") + 
  labs(x = "Dato", y = "Miliarder av Euro", title = "Total støtte sendt til Ukrania i Miliarder av Euro") +
  theme_minimal() + 
  labs(caption = "Kilde: Trebesch et al. (2023) Kiel Working Paper The Ukraine Support Tracker")


ggplot(filtered_data, aes(x = Month, y = `Military aid, allocated (€ billion)`)) + 
  geom_bar(stat = "identity") + 
  labs(x = "Dato", y = "Miliarder av Euro", title = "Militærstøtte sendt til Ukrania i Miliarder av Euro") +
  theme_minimal() + 
  labs(caption = "Kilde: Trebesch et al. (2023) Kiel Working Paper The Ukraine Support Tracker")

# In case "Month" in filtered_data is not a Date object, convert it first:
filtered_data$Month <- as.Date(filtered_data$Month)

# Create a new column in filtered_data with just the year and month in character format:
filtered_data$month_year_to_join <- format(filtered_data$Month, "%Y-%m")

ggplot(joined_data, aes(x = Month, y = losses_per_day)) + 
geom_bar(stat = "identity") + 
  labs(x = "Måned", y = "Tapstall per måned", subtitle = "februar 2023 - mars 2024", caption = "Kilde: Ivanouk 2022") + 
  ggtitle("Tapstall over russiske soldater i Ukrania") + 
  theme_minimal() 

library(ggplot2)
library(gridExtra)
library(grid)

# Save the primary caption to a variable since it's the same for both plots
common_caption <- "Kilde: Trebesch et al. (2023) Kiel Working Paper The Ukraine Support Tracker"

# Create the first plot with its specific subtitle
plot1 <- ggplot(filtered_data, aes(x = Month, y = `Total aid, allocated (€ billion)`)) +
  geom_bar(stat = "identity") + 
  labs(x = "Dato", y = "Miliarder av Euro", subtitle = "Total støtte") +
  theme_minimal()

# Create the second plot with its specific subtitle
plot2 <- ggplot(filtered_data, aes(x = Month, y = `Military aid, allocated (€ billion)`)) + 
  geom_bar(stat = "identity") + 
  labs(x = "Dato", y = "Miliarder av Euro", subtitle = "Militærstøtte") +
  theme_minimal()

# Define the size for the main title and other text elements
title_size <- 16
subtitle_size <- 12
caption_size <- 10

# Use grid.arrange to place the plots side by side with a customized main title and caption
grid.arrange(
  plot1, plot2, 
  ncol = 2,
  top = textGrob("Monetær støtte til Ukraina", gp = gpar(fontsize = title_size, fontface = "bold")),
  bottom = textGrob(common_caption, gp = gpar(fontsize = caption_size))
)



joined_data <- Negative_losses %>%
  left_join(filtered_data, by = c("month_year" = "month_year_to_join"))




library(ggplot2)
library(gridExtra)

plot4 <- ggplot(filtered_data, aes(x = Month, y = `Total aid, allocated (€ billion)`)) +
  geom_line() + 
  labs(x = "Dato", y = "Miliarder av Euro", subtitle = "Total støtte") +
  theme_minimal()

plot3 <- ggplot(data_negative, aes(x = month_year, y = percentage, group = "Negative")) +
  geom_line(color = "red") +
  labs(x = "", y = "Prosent av negative artikler") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  ggtitle("Prosentandel negative nyhetsreportasjer om krigen i Ukraina", subtitle = "Februar 2023 - Mars 2024") + 
  labs(caption = "Hentet fra den russiske statseide avisen Izvestia")



# Arrange the two plots side by side
grid.arrange(plot4, plot3, ncol = 2)

```

```{r}

clean_joined <- na.omit(joined_data)


clean_joined <- clean_joined %>%
  rename(Aid = `Total aid, allocated (€ billion)`)

clean_joined <- clean_joined %>%
  rename(Military_Aid = `Military aid, allocated (€ billion)`)

is.numeric(clean_joined$Aid)



library(ggplot2)

# Assuming you have a scale_factor that's properly defined for your data
# scale_factor will be used to create the correct relationship between 'Aid' and 'percentage'
# For example, if your 'Aid' column is in billions and 'percentage' is actually a percentage, you might do something like:
scale_factor <- max(clean_joined$Aid) / 40  # Adapting the 'percentage' to the 'Aid' scale
inverse_scale_factor <- 1 / scale_factor  # The inverse for the secondary axis

ggplot(data = clean_joined, aes(x = month_year)) +
  geom_col(aes(y = Aid, fill = "Aid")) +
  geom_line(aes(y = percentage * scale_factor, color = "Percentage", group = 1)) +
  scale_fill_manual(name = "", values = "skyblue", labels = "Støtte til Ukraina i\n Milliarder av Euro") +
  scale_color_manual(name = "", values = "red", labels = "Prosent negative \nnyheter") +
  scale_y_continuous(
    name = "Støtte i miliarder av Euro",  # Left y-axis for 'Aid'
    sec.axis = sec_axis(~ . * inverse_scale_factor, name = "Prosent")  # Right y-axis for 'percentage'
  ) +
  theme_minimal() +
  labs(
    x = "",
    y = " Støtte i miliarder Euro",
    title = "Total støttepakker sendt til Ukraina og prosentandel \nnegative nyheter i Izvestia",
    subtitle = "Dual y-axis plot", 
    caption = "Kilde: Trebesch, et. al. 2024 & Izvestia.ru"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(
    fill = guide_legend(title = "", order = 1), 
    color = guide_legend(title = "", order = 2)
  )




scale_factor <- max(clean_joined$Military_Aid) / 40  # Adapting the 'percentage' to the 'Aid' scale
inverse_scale_factor <- 1 / scale_factor  # The inverse for the secondary axis

ggplot(data = clean_joined, aes(x = month_year)) +
  geom_col(aes(y = Military_Aid, fill = "Military_Aid")) +
  geom_line(aes(y = percentage * scale_factor, color = "Percentage", group = 1)) +
  scale_fill_manual(name = "", values = "darkgreen", labels = "Militærstøtte til Ukraina i\n Milliarder av Euro") +
  scale_color_manual(name = "", values = "red", labels = "Prosent negative \nnyheter") +
  scale_y_continuous(
    name = "Støtte i miliarder av Euro",  # Left y-axis for 'Aid'
    sec.axis = sec_axis(~ . * inverse_scale_factor, name = "Prosent")  # Right y-axis for 'percentage'
  ) +
  theme_minimal() +
  labs(
    x = "",
    y = " Støtte i miliarder Euro",
    title = "Militærstøtte sendt til Ukraina og prosentandel negative nyheter i Izvestia",
    subtitle = "Dual y-axis plot", 
    caption = "Kilde: Trebesch, et. al. 2024 & Izvestia.ru"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(
    fill = guide_legend(title = "", order = 1), 
    color = guide_legend(title = "", order = 2)
  )

reg_caption <- "Kilde: Trebesch, et. al. 2024 & Izvestia.ru"

regplot1 <- ggplot(joined_data, aes(x = `Total aid, allocated (€ billion)`, y = percentage)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_minimal() + 
  labs( x = "Total støtte sendt til Ukraina", 
        y = "Prosentandel negative nyheter")


regplot2 <- ggplot(joined_data, aes(x = `Military aid, allocated (€ billion)`, y = percentage)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_minimal() + 
  labs( x = "Militær støtte sendt til Ukraina", 
        y = "Prosentandel negative nyheter")

# Define the size for the main title and other text elements
title_size <- 14
subtitle_size <- 10
caption_size <- 10

# Use grid.arrange to place the plots side by side with a customized main title and caption
grid.arrange(
  regplot1, regplot2, 
  ncol = 2,
  top = textGrob("Finansiell og militær støtte sendt til Ukraina og\n prosentandel negative nyheter", gp = gpar(fontsize = title_size, fontface = "bold")),
  bottom = textGrob(reg_caption, gp = gpar(fontsize = caption_size))
)

lm_støtte <- lm(data = joined_data,  percentage ~ joined_data$`Total aid, allocated (€ billion)`)


# Run the model as you mentioned in your code
lm_støtte <- lm(percentage ~ `Total aid, allocated (€ billion)`, data = joined_data)


summary(lm_støtte)

```
